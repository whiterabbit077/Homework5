[
  {
    "objectID": "hw5.html",
    "href": "hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "1. What is the purpose of using cross-validation when fitting a random forest model?\n&gt; Cross-validation is used in random forests to choose the best tuning parameter, like\nthe number of trees. By dividing the data into k-folds, the model is trained on k-1 folds\nand validated on the remaining fold. This process is repeated for each fold, and the \nperformance is averaged. This helps ensure that the chosen parameter value allows the \nmodel to generalize well to new, unseen data, rather than just memorizing the training \ndata.\n\n2. Describe the bagged tree algorithm.\n&gt; Bagging, or Bootstrap Aggregating, is a way to improve the stability and accuracy \nof machine learning algorithms. For bagged trees, you take multiple samples from your\ndata (with replacement) and build a decision tree for each sample. Then, you combine \nthe predictions from all these trees. It's like asking multiple friends for their \nopinions before making a decision – you get a more balanced and reliable outcome.\n\n3. What is meant by a general linear model?\n&gt; A general linear model is a statistical technique that models the relationship between\none dependent variable and one or more independent variables. The model assumes that the\nrelationship between the dependent variable and the independent variables is linear. This\nincludes models like for example simple linear regression or multiple linear regression.\n\n4. When fitting a multiple linear regression model, what does adding an interaction term do? \nThat is, what does it allow the model to do differently as compared to when it is not \nincluded in the model?\n&gt; Adding an interaction term lets your model capture the combined effect of two variables\non the outcome, beyond their individual effects. It allows the model to consider that the\nrelationship between variables isn't just additive but can be more complex.\n\n5. Why do we split our data into a training and test set?\n&gt; Splitting your data into training and test sets helps you evaluate how well your model\nwill perform on new data. The training set is for building the model, while the test set\nis like a final exam to see how well it learned. This way, you can check if your model\ncan generalize well to new data and isn't just overfitting to the training data."
  },
  {
    "objectID": "hw5.html#task-1-conceptual-questions",
    "href": "hw5.html#task-1-conceptual-questions",
    "title": "Homework 5",
    "section": "",
    "text": "1. What is the purpose of using cross-validation when fitting a random forest model?\n&gt; Cross-validation is used in random forests to choose the best tuning parameter, like\nthe number of trees. By dividing the data into k-folds, the model is trained on k-1 folds\nand validated on the remaining fold. This process is repeated for each fold, and the \nperformance is averaged. This helps ensure that the chosen parameter value allows the \nmodel to generalize well to new, unseen data, rather than just memorizing the training \ndata.\n\n2. Describe the bagged tree algorithm.\n&gt; Bagging, or Bootstrap Aggregating, is a way to improve the stability and accuracy \nof machine learning algorithms. For bagged trees, you take multiple samples from your\ndata (with replacement) and build a decision tree for each sample. Then, you combine \nthe predictions from all these trees. It's like asking multiple friends for their \nopinions before making a decision – you get a more balanced and reliable outcome.\n\n3. What is meant by a general linear model?\n&gt; A general linear model is a statistical technique that models the relationship between\none dependent variable and one or more independent variables. The model assumes that the\nrelationship between the dependent variable and the independent variables is linear. This\nincludes models like for example simple linear regression or multiple linear regression.\n\n4. When fitting a multiple linear regression model, what does adding an interaction term do? \nThat is, what does it allow the model to do differently as compared to when it is not \nincluded in the model?\n&gt; Adding an interaction term lets your model capture the combined effect of two variables\non the outcome, beyond their individual effects. It allows the model to consider that the\nrelationship between variables isn't just additive but can be more complex.\n\n5. Why do we split our data into a training and test set?\n&gt; Splitting your data into training and test sets helps you evaluate how well your model\nwill perform on new data. The training set is for building the model, while the test set\nis like a final exam to see how well it learned. This way, you can check if your model\ncan generalize well to new data and isn't just overfitting to the training data."
  },
  {
    "objectID": "hw5.html#task-2-fitting-models",
    "href": "hw5.html#task-2-fitting-models",
    "title": "Homework 5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\nRead the data\n\n# Load required packages\nlibrary(readr)\nlibrary(httr)\n\n# Define the URL\nurl &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/heart.csv\"\n\n# Read the CSV file directly from the URL\nheart_data &lt;- read_csv(url)\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows of the data\nhead(heart_data)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nQuick EDA/Data Preparation\n\n# Load necessary libraries\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:httr':\n\n    progress\n\nlibrary(ggplot2)\n\n# assign heart data to object data\ndata &lt;- heart_data\n\n#Step 1: Quickly understand the data\n# Check for missingness\nsummary(data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\nsapply(data, function(x) sum(is.na(x)))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n#no missing values\n\n# Summarize data\n# Relationships of variables to HeartDisease\ntable(data$HeartDisease)\n\n\n  0   1 \n410 508 \n\n# Numeric variables\n# Identify numeric columns\nnumeric_vars &lt;- sapply(data, is.numeric)\nnumeric_cols &lt;- names(data)[numeric_vars]\n\n# Create boxplots for all numeric variables against HeartDisease\nfor (col in numeric_cols) {\n  if (col != \"HeartDisease\") {\n    boxplot_formula &lt;- as.formula(paste(col, \"~ HeartDisease\"))\n    boxplot(boxplot_formula, data = data, main = paste(col, \"vs HeartDisease\"))\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Categorical variables\n# Identify categorical columns\ncategorical_vars &lt;- sapply(data, is.character)\ncategorical_cols &lt;- names(data)[categorical_vars]\n\n# Loop through categorical columns and create ggplot bar plots\nfor (col in categorical_cols) {\n  if (col != \"HeartDisease\") {\n    p &lt;- ggplot(data, aes(x = as.factor(.data[[col]]), fill = as.factor(HeartDisease))) +\n      geom_bar(position = \"dodge\") +\n      labs(title = paste(col, \"vs HeartDisease\"), x = col, y = \"Count\") +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n    print(p)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Step 2: Create a Factor Version of HeartDisease Variable\ndata$HeartDiseaseFactor &lt;- as.factor(data$HeartDisease)\nstr(data)\n\nspc_tbl_ [918 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Age               : num [1:918] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex               : chr [1:918] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType     : chr [1:918] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP         : num [1:918] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol       : num [1:918] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS         : num [1:918] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG        : chr [1:918] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR             : num [1:918] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina    : chr [1:918] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak           : num [1:918] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope          : chr [1:918] \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease      : num [1:918] 0 1 0 1 0 0 0 0 1 0 ...\n $ HeartDiseaseFactor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Age = col_double(),\n  ..   Sex = col_character(),\n  ..   ChestPainType = col_character(),\n  ..   RestingBP = col_double(),\n  ..   Cholesterol = col_double(),\n  ..   FastingBS = col_double(),\n  ..   RestingECG = col_character(),\n  ..   MaxHR = col_double(),\n  ..   ExerciseAngina = col_character(),\n  ..   Oldpeak = col_double(),\n  ..   ST_Slope = col_character(),\n  ..   HeartDisease = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Remove ST_Slope and original HeartDisease variables\ndata &lt;- data[, !(names(data) %in% c(\"ST_Slope\", \"HeartDisease\"))]\nstr(data)\n\ntibble [918 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age               : num [1:918] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex               : chr [1:918] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType     : chr [1:918] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP         : num [1:918] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol       : num [1:918] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS         : num [1:918] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG        : chr [1:918] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR             : num [1:918] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina    : chr [1:918] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak           : num [1:918] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDiseaseFactor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n\n# Step 3: Create Dummy Variables for Categorical Predictors\n# Define var list for dummyVars\ndummy_formula &lt;- ~ Sex + ExerciseAngina + ChestPainType + RestingECG\n\n# Create the dummyVars object\ndummies &lt;- dummyVars(dummy_formula, data = data)\n\n# Use predict to create new columns\ndummy_data &lt;- predict(dummies, newdata = data)\n\n# Convert to data frame and add to original data\ndummy_data &lt;- as.data.frame(dummy_data)\ndata &lt;- cbind(data, dummy_data)\n\n# Drop original categorical columns\ndata &lt;- data[, !(names(data) %in% c(\"Sex\", \"ExerciseAngina\", \"ChestPainType\", \"RestingECG\"))]\nstr(data)\n\n'data.frame':   918 obs. of  18 variables:\n $ Age               : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP         : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol       : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS         : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR             : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak           : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDiseaseFactor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n $ SexF              : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM              : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ExerciseAnginaN   : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY   : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeASY  : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA  : num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP  : num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal  : num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST      : num  0 0 1 0 0 0 0 0 0 0 ...\n\n\n\n\nData Split\n\n# Set a seed for reproducibility\nset.seed(1234)\n\n# Create indices for the training set\ntrainIndex &lt;- createDataPartition(data$HeartDisease, p = 0.7, list = FALSE)\n\n# Split the data into training and test sets\ntrainData &lt;- data[trainIndex, ]\ntestData &lt;- data[-trainIndex, ]\n\n# Check the dimensions of the splits\ndim(trainData)\n\n[1] 643  18\n\ndim(testData)\n\n[1] 275  18\n\n\n\n\nkNN\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", \n                       number = 10, \n                       repeats = 3)\n\n# Define the tuning grid\ntune_grid &lt;- expand.grid(k = 1:40)\n\nset.seed(1234)\nknn_fit &lt;- train(HeartDiseaseFactor ~.,\n                 data = trainData, \n                 method = \"knn\",\n trControl=trctrl,\n preProcess = c(\"center\", \"scale\"),\n tuneGrid = tune_grid)\n                              \n\n# Make predictions on the test set\npredictions &lt;- predict(knn_fit, newdata = testData)\n\n# Evaluate the model\nknn_conf_matrix &lt;- confusionMatrix(predictions, testData$HeartDiseaseFactor)\nprint(knn_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  96  32\n         1  27 120\n                                          \n               Accuracy : 0.7855          \n                 95% CI : (0.7322, 0.8325)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 6.355e-16       \n                                          \n                  Kappa : 0.5678          \n                                          \n Mcnemar's Test P-Value : 0.6025          \n                                          \n            Sensitivity : 0.7805          \n            Specificity : 0.7895          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.8163          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3491          \n   Detection Prevalence : 0.4655          \n      Balanced Accuracy : 0.7850          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nLogistic Regression\n\nset.seed(1234)\n# Set up the train control\ntrain_control &lt;- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 3)\n\n# Model 1: All predictors\nmodel1 &lt;- train(HeartDiseaseFactor ~ ., \n                data = trainData, \n                method = \"glm\", \n                family = \"binomial\",\n                preProcess = c(\"center\", \"scale\"),\n                trControl = train_control)\n\n# Model 2: A subset of predictors\nmodel2 &lt;- train(HeartDiseaseFactor ~ Age + RestingBP + Cholesterol, \n                data = trainData, \n                method = \"glm\", \n                family = \"binomial\",\n                preProcess = c(\"center\", \"scale\"),\n                trControl = train_control)\n\n# Model 3: Another subset of predictors\nmodel3 &lt;- train(HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + FastingBS + MaxHR, \n                data = trainData, \n                method = \"glm\", \n                family = \"binomial\",\n                preProcess = c(\"center\", \"scale\"),\n                trControl = train_control)\n\n# Compare models\nresults &lt;- resamples(list(model1 = model1, model2 = model2, model3 = model3))\nsummary(results)\n\n\nCall:\nsummary.resamples(object = results)\n\nModels: model1, model2, model3 \nNumber of resamples: 30 \n\nAccuracy \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.7384615 0.8000000 0.8153846 0.8250046 0.8663004 0.9047619    0\nmodel2 0.5625000 0.6562500 0.6743990 0.6733145 0.7159856 0.7538462    0\nmodel3 0.6250000 0.6783272 0.7054087 0.7034874 0.7187500 0.8593750    0\n\nKappa \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.4760550 0.5959535 0.6276808 0.6453828 0.7259933 0.8085106    0\nmodel2 0.1098901 0.3012033 0.3300777 0.3316064 0.4151451 0.4951456    0\nmodel3 0.2258065 0.3498834 0.4039843 0.3974232 0.4235667 0.7170923    0\n\ndotplot(results)\n\n\n\n#From the summary, Model 1 consistently has higher values for both accuracy and kappa compared to Models 2 and 3. Specifically. Therefore, Model 1 is the best model based on these evaluation metrics.\n\n# Since model1 is the best model\npredictions &lt;- predict(model1, newdata = testData)\n\n# Evaluate the best model\nlg_conf_matrix &lt;- confusionMatrix(predictions, testData$HeartDiseaseFactor)\nprint(lg_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  95  24\n         1  28 128\n                                          \n               Accuracy : 0.8109          \n                 95% CI : (0.7595, 0.8554)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6164          \n                                          \n Mcnemar's Test P-Value : 0.6774          \n                                          \n            Sensitivity : 0.7724          \n            Specificity : 0.8421          \n         Pos Pred Value : 0.7983          \n         Neg Pred Value : 0.8205          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3455          \n   Detection Prevalence : 0.4327          \n      Balanced Accuracy : 0.8072          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nTree models\n\nlibrary(rpart)\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(gbm)\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nset.seed(1234)\n#Train classification tree model\nctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\ngrid_cp &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\nrpart_model1 &lt;- train(HeartDiseaseFactor ~ ., data = trainData, \n                     method = \"rpart\", \n                     trControl = ctrl, \n                     tuneGrid = grid_cp)\n\nprint(rpart_model1)\n\nCART \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 578, 579, 579, 579, 578, 580, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7960284  0.5872624\n  0.001  0.7960284  0.5872624\n  0.002  0.7955236  0.5863683\n  0.003  0.7960525  0.5878335\n  0.004  0.8022709  0.6006511\n  0.005  0.8022709  0.6006511\n  0.006  0.8027675  0.6015429\n  0.007  0.8017418  0.5996597\n  0.008  0.7954836  0.5872539\n  0.009  0.7954836  0.5873397\n  0.010  0.7965092  0.5896372\n  0.011  0.7944416  0.5856618\n  0.012  0.7918775  0.5806541\n  0.013  0.7918775  0.5806541\n  0.014  0.7908519  0.5787132\n  0.015  0.7903714  0.5775728\n  0.016  0.7846820  0.5664620\n  0.017  0.7805068  0.5571022\n  0.018  0.7825661  0.5611126\n  0.019  0.7783909  0.5507371\n  0.020  0.7784072  0.5503926\n  0.021  0.7794409  0.5519861\n  0.022  0.7846016  0.5602581\n  0.023  0.7846016  0.5602581\n  0.024  0.7851145  0.5618499\n  0.025  0.7851145  0.5618499\n  0.026  0.7851145  0.5618499\n  0.027  0.7851145  0.5618499\n  0.028  0.7815327  0.5541964\n  0.029  0.7815327  0.5541964\n  0.030  0.7815327  0.5541964\n  0.031  0.7804828  0.5519597\n  0.032  0.7794411  0.5497726\n  0.033  0.7794411  0.5497726\n  0.034  0.7794411  0.5497726\n  0.035  0.7779027  0.5471592\n  0.036  0.7779027  0.5471592\n  0.037  0.7779027  0.5471592\n  0.038  0.7779027  0.5471592\n  0.039  0.7753786  0.5432945\n  0.040  0.7753786  0.5432945\n  0.041  0.7753786  0.5432945\n  0.042  0.7753786  0.5432945\n  0.043  0.7733033  0.5398077\n  0.044  0.7733033  0.5398077\n  0.045  0.7733033  0.5398077\n  0.046  0.7733033  0.5398077\n  0.047  0.7702104  0.5347595\n  0.048  0.7702104  0.5347595\n  0.049  0.7702104  0.5347595\n  0.050  0.7702104  0.5347595\n  0.051  0.7707232  0.5369934\n  0.052  0.7707232  0.5369934\n  0.053  0.7707232  0.5369934\n  0.054  0.7707232  0.5369934\n  0.055  0.7717814  0.5400334\n  0.056  0.7717814  0.5400334\n  0.057  0.7717814  0.5400334\n  0.058  0.7744021  0.5456648\n  0.059  0.7744021  0.5456648\n  0.060  0.7744021  0.5456648\n  0.061  0.7744021  0.5456648\n  0.062  0.7754603  0.5479180\n  0.063  0.7765020  0.5501335\n  0.064  0.7765020  0.5501335\n  0.065  0.7765020  0.5501335\n  0.066  0.7765020  0.5501335\n  0.067  0.7765020  0.5501335\n  0.068  0.7765020  0.5501335\n  0.069  0.7765020  0.5501335\n  0.070  0.7765020  0.5501335\n  0.071  0.7765020  0.5501335\n  0.072  0.7765020  0.5501335\n  0.073  0.7765020  0.5501335\n  0.074  0.7765020  0.5501335\n  0.075  0.7765020  0.5501335\n  0.076  0.7765020  0.5501335\n  0.077  0.7765020  0.5501335\n  0.078  0.7765020  0.5501335\n  0.079  0.7765020  0.5501335\n  0.080  0.7765020  0.5501335\n  0.081  0.7765020  0.5501335\n  0.082  0.7765020  0.5501335\n  0.083  0.7765020  0.5501335\n  0.084  0.7765020  0.5501335\n  0.085  0.7765020  0.5501335\n  0.086  0.7765020  0.5501335\n  0.087  0.7765020  0.5501335\n  0.088  0.7765020  0.5501335\n  0.089  0.7765020  0.5501335\n  0.090  0.7765020  0.5501335\n  0.091  0.7765020  0.5501335\n  0.092  0.7765020  0.5501335\n  0.093  0.7765020  0.5501335\n  0.094  0.7765020  0.5501335\n  0.095  0.7765020  0.5501335\n  0.096  0.7765020  0.5501335\n  0.097  0.7765020  0.5501335\n  0.098  0.7765020  0.5501335\n  0.099  0.7765020  0.5501335\n  0.100  0.7765020  0.5501335\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.006.\n\nrpart_model2 &lt;- train(HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR, data = trainData, \n                     method = \"rpart\", \n                     trControl = ctrl, \n                     tuneGrid = grid_cp)\n\nprint(rpart_model2)\n\nCART \n\n643 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 580, 580, 578, 578, 579, 579, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.6987588  0.3906431\n  0.001  0.7002972  0.3942524\n  0.002  0.7034303  0.4016177\n  0.003  0.7034222  0.4011892\n  0.004  0.7028688  0.3992103\n  0.005  0.7044313  0.4025582\n  0.006  0.7018354  0.3940569\n  0.007  0.6987182  0.3868042\n  0.008  0.7018106  0.3930872\n  0.009  0.7033731  0.3956140\n  0.010  0.7075563  0.4032755\n  0.011  0.7075563  0.4027719\n  0.012  0.7086223  0.4041327\n  0.013  0.7096479  0.4051603\n  0.014  0.7137745  0.4145323\n  0.015  0.7148162  0.4165801\n  0.016  0.7179412  0.4221008\n  0.017  0.7179412  0.4221008\n  0.018  0.7168910  0.4194083\n  0.019  0.7179327  0.4211711\n  0.020  0.7174284  0.4199040\n  0.021  0.7174284  0.4199040\n  0.022  0.7184540  0.4204074\n  0.023  0.7184540  0.4204074\n  0.024  0.7205454  0.4234783\n  0.025  0.7205454  0.4234783\n  0.026  0.7205454  0.4234783\n  0.027  0.7205454  0.4234783\n  0.028  0.7190069  0.4201066\n  0.029  0.7190069  0.4201066\n  0.030  0.7169236  0.4155367\n  0.031  0.7169236  0.4155367\n  0.032  0.7169236  0.4155367\n  0.033  0.7169236  0.4155367\n  0.034  0.7169236  0.4155367\n  0.035  0.7127810  0.4072369\n  0.036  0.7127810  0.4072369\n  0.037  0.7112425  0.4034709\n  0.038  0.7076528  0.3954047\n  0.039  0.7050646  0.3903272\n  0.040  0.7050646  0.3903272\n  0.041  0.7045518  0.3888701\n  0.042  0.7045518  0.3888701\n  0.043  0.6977970  0.3765951\n  0.044  0.6977970  0.3765951\n  0.045  0.6977970  0.3765951\n  0.046  0.6977970  0.3765951\n  0.047  0.6936624  0.3691322\n  0.048  0.6936624  0.3691322\n  0.049  0.6936624  0.3691322\n  0.050  0.6936624  0.3691322\n  0.051  0.6879081  0.3587007\n  0.052  0.6879081  0.3587007\n  0.053  0.6879081  0.3587007\n  0.054  0.6879081  0.3587007\n  0.055  0.6884290  0.3598587\n  0.056  0.6884290  0.3598587\n  0.057  0.6884290  0.3598587\n  0.058  0.6884290  0.3598587\n  0.059  0.6884290  0.3598587\n  0.060  0.6884290  0.3598587\n  0.061  0.6884290  0.3598587\n  0.062  0.6884290  0.3598587\n  0.063  0.6884290  0.3598587\n  0.064  0.6884290  0.3598587\n  0.065  0.6884290  0.3598587\n  0.066  0.6884290  0.3598587\n  0.067  0.6884290  0.3598587\n  0.068  0.6884290  0.3598587\n  0.069  0.6884290  0.3598587\n  0.070  0.6884290  0.3598587\n  0.071  0.6884290  0.3598587\n  0.072  0.6884290  0.3598587\n  0.073  0.6884290  0.3598587\n  0.074  0.6884290  0.3598587\n  0.075  0.6884290  0.3598587\n  0.076  0.6884290  0.3598587\n  0.077  0.6884290  0.3598587\n  0.078  0.6884290  0.3598587\n  0.079  0.6884290  0.3598587\n  0.080  0.6884290  0.3598587\n  0.081  0.6884290  0.3598587\n  0.082  0.6847831  0.3532435\n  0.083  0.6847831  0.3532435\n  0.084  0.6847831  0.3532435\n  0.085  0.6847831  0.3532435\n  0.086  0.6847831  0.3532435\n  0.087  0.6847831  0.3532435\n  0.088  0.6847831  0.3532435\n  0.089  0.6811208  0.3475022\n  0.090  0.6785567  0.3430415\n  0.091  0.6785567  0.3430415\n  0.092  0.6785567  0.3430415\n  0.093  0.6780358  0.3422194\n  0.094  0.6733884  0.3348251\n  0.095  0.6733884  0.3348251\n  0.096  0.6733884  0.3348251\n  0.097  0.6718259  0.3322982\n  0.098  0.6718259  0.3322982\n  0.099  0.6718259  0.3322982\n  0.100  0.6718259  0.3322982\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.027.\n\n#Train a random Forest Model\ngrid_rf &lt;- expand.grid(mtry = 1:(ncol(trainData)-1))\n\n#this takes way too long to compute\n#rf_model1 &lt;- train(HeartDiseaseFactor ~ ., data = trainData, \n#                  method = \"rf\", \n#                  trControl = ctrl, \n#                  tuneGrid = grid_rf)\n\n#print(rf_model1)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ stringr   1.5.1\n✔ forcats   1.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::combine()       masks randomForest::combine()\n✖ dplyr::filter()        masks stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ purrr::lift()          masks caret::lift()\n✖ randomForest::margin() masks ggplot2::margin()\n✖ caret::progress()      masks httr::progress()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ngrid_rf2 &lt;- expand.grid(mtry = 1:(ncol(trainData |&gt; select(HeartDiseaseFactor, Age, RestingBP, Cholesterol, MaxHR))-1))\n\nrf_model2 &lt;- train(HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR, data = trainData, \n                  method = \"rf\", \n                  trControl = ctrl, \n                  tuneGrid = grid_rf2)\n\nprint(rf_model2)\n\nRandom Forest \n\n643 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 579, 579, 578, 580, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.7144259  0.4186904\n  2     0.7190808  0.4296139\n  3     0.7091915  0.4099162\n  4     0.7066116  0.4056595\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n#Train a boosted Tree Model\ngrid_gbm &lt;- expand.grid(n.trees = c(25, 50, 100, 200),\n                        interaction.depth = c(1, 2, 3),\n                        shrinkage = 0.1,\n                        n.minobsinnode = 10)\n\ngbm_model1 &lt;- train(HeartDiseaseFactor ~ ., data = trainData, \n                   method = \"gbm\", \n                   trControl = ctrl, \n                   tuneGrid = grid_gbm, \n                   verbose = FALSE)\n\nprint(gbm_model1)\n\nStochastic Gradient Boosting \n\n643 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 578, 578, 578, 578, 580, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8066486  0.6073921\n  1                   50      0.8237249  0.6429110\n  1                  100      0.8414193  0.6794047\n  1                  200      0.8477277  0.6922714\n  2                   25      0.8232847  0.6408247\n  2                   50      0.8393447  0.6744529\n  2                  100      0.8404350  0.6768655\n  2                  200      0.8384809  0.6732450\n  3                   25      0.8279730  0.6510896\n  3                   50      0.8419492  0.6796675\n  3                  100      0.8455715  0.6870079\n  3                  200      0.8300729  0.6554239\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 200, interaction.depth =\n 1, shrinkage = 0.1 and n.minobsinnode = 10.\n\ngbm_model2 &lt;- train(HeartDiseaseFactor ~ Age + RestingBP + Cholesterol + MaxHR, data = trainData, \n                   method = \"gbm\", \n                   trControl = ctrl, \n                   tuneGrid = grid_gbm, \n                   verbose = FALSE)\n\nprint(gbm_model2)\n\nStochastic Gradient Boosting \n\n643 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 579, 579, 578, 579, 579, 580, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7242752  0.4380593\n  1                   50      0.7247955  0.4390402\n  1                  100      0.7201804  0.4302571\n  1                  200      0.7211809  0.4330727\n  2                   25      0.7320391  0.4518432\n  2                   50      0.7346680  0.4583598\n  2                  100      0.7279127  0.4464591\n  2                  200      0.7122940  0.4154406\n  3                   25      0.7284007  0.4433571\n  3                   50      0.7299395  0.4486524\n  3                  100      0.7164849  0.4239004\n  3                  200      0.6983757  0.3872855\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n#Evaluate models on the TEST set\n# Predictions\n#full model\nrpart_preds1 &lt;- predict(rpart_model1, newdata = testData)\n#rf_preds1 &lt;- predict(rf_model1, newdata = testData)\ngbm_preds1 &lt;- predict(gbm_model1, newdata = testData)\n#models with selected variables\nrpart_preds2 &lt;- predict(rpart_model2, newdata = testData)\nrf_preds2 &lt;- predict(rf_model2, newdata = testData)\ngbm_preds2 &lt;- predict(gbm_model2, newdata = testData)\n\n# Confusion Matrices\nrpart_cm1 &lt;- confusionMatrix(rpart_preds1, testData$HeartDiseaseFactor)\n#rf_cm1 &lt;- confusionMatrix(rf_preds1, testData$HeartDiseaseFactor)\ngbm_cm1 &lt;- confusionMatrix(gbm_preds1, testData$HeartDiseaseFactor)\nrpart_cm2 &lt;- confusionMatrix(rpart_preds2, testData$HeartDiseaseFactor)\nrf_cm2 &lt;- confusionMatrix(rf_preds2, testData$HeartDiseaseFactor)\ngbm_cm2 &lt;- confusionMatrix(gbm_preds2, testData$HeartDiseaseFactor)\n\nprint(rpart_cm1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  93  30\n         1  30 122\n                                          \n               Accuracy : 0.7818          \n                 95% CI : (0.7283, 0.8292)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 1.871e-15       \n                                          \n                  Kappa : 0.5587          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7561          \n            Specificity : 0.8026          \n         Pos Pred Value : 0.7561          \n         Neg Pred Value : 0.8026          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3382          \n   Detection Prevalence : 0.4473          \n      Balanced Accuracy : 0.7794          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#print(rf_cm1)\nprint(gbm_cm1)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  96  26\n         1  27 126\n                                          \n               Accuracy : 0.8073          \n                 95% CI : (0.7556, 0.8522)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6099          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7805          \n            Specificity : 0.8289          \n         Pos Pred Value : 0.7869          \n         Neg Pred Value : 0.8235          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3491          \n   Detection Prevalence : 0.4436          \n      Balanced Accuracy : 0.8047          \n                                          \n       'Positive' Class : 0               \n                                          \n\nprint(rpart_cm2)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  75  29\n         1  48 123\n                                          \n               Accuracy : 0.72            \n                 95% CI : (0.6629, 0.7723)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 8.572e-09       \n                                          \n                  Kappa : 0.4252          \n                                          \n Mcnemar's Test P-Value : 0.04024         \n                                          \n            Sensitivity : 0.6098          \n            Specificity : 0.8092          \n         Pos Pred Value : 0.7212          \n         Neg Pred Value : 0.7193          \n             Prevalence : 0.4473          \n         Detection Rate : 0.2727          \n   Detection Prevalence : 0.3782          \n      Balanced Accuracy : 0.7095          \n                                          \n       'Positive' Class : 0               \n                                          \n\nprint(rf_cm2)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  84  42\n         1  39 110\n                                          \n               Accuracy : 0.7055          \n                 95% CI : (0.6477, 0.7587)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 1.466e-07       \n                                          \n                  Kappa : 0.4057          \n                                          \n Mcnemar's Test P-Value : 0.8241          \n                                          \n            Sensitivity : 0.6829          \n            Specificity : 0.7237          \n         Pos Pred Value : 0.6667          \n         Neg Pred Value : 0.7383          \n             Prevalence : 0.4473          \n         Detection Rate : 0.3055          \n   Detection Prevalence : 0.4582          \n      Balanced Accuracy : 0.7033          \n                                          \n       'Positive' Class : 0               \n                                          \n\nprint(gbm_cm2)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  81  38\n         1  42 114\n                                          \n               Accuracy : 0.7091          \n                 95% CI : (0.6515, 0.7621)\n    No Information Rate : 0.5527          \n    P-Value [Acc &gt; NIR] : 7.397e-08       \n                                          \n                  Kappa : 0.4098          \n                                          \n Mcnemar's Test P-Value : 0.7373          \n                                          \n            Sensitivity : 0.6585          \n            Specificity : 0.7500          \n         Pos Pred Value : 0.6807          \n         Neg Pred Value : 0.7308          \n             Prevalence : 0.4473          \n         Detection Rate : 0.2945          \n   Detection Prevalence : 0.4327          \n      Balanced Accuracy : 0.7043          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nCompare models\n\n# Extracting accuracy for comparison\n\nknn_accuracy &lt;- knn_conf_matrix$overall['Accuracy']\nlogreg_accuracy &lt;- lg_conf_matrix$overall['Accuracy']\nrpart_accuracy1 &lt;- rpart_cm1$overall['Accuracy']\nrpart_accuracy2 &lt;- rpart_cm2$overall['Accuracy']\n#rf_accuracy1 &lt;- rf_cm1$overall['Accuracy']\nrf_accuracy2 &lt;- rf_cm2$overall['Accuracy']\ngbm_accuracy1 &lt;- gbm_cm1$overall['Accuracy']\ngbm_accuracy2 &lt;- gbm_cm2$overall['Accuracy']\n\n# Combine accuracies into a data frame for easy comparison\naccuracy_results &lt;- data.frame(\n  Model = c(\"kNN\", \n            \"Logistic Regression\", \n            \"rpart full\", \"rpart slected\",\n            #\"Random Forest full\", \n            \"Random Forest selected\",\n            \"GBM full\", \"GBM selected\"),\n  Accuracy = c(knn_accuracy, \n               logreg_accuracy, \n               rpart_accuracy1, rpart_accuracy2,  \n               #rf_accuracy1, \n               rf_accuracy2,  \n               gbm_accuracy1, gbm_accuracy2)\n)\n\n# Print the results\nprint(accuracy_results)\n\n                   Model  Accuracy\n1                    kNN 0.7854545\n2    Logistic Regression 0.8109091\n3             rpart full 0.7818182\n4          rpart slected 0.7200000\n5 Random Forest selected 0.7054545\n6               GBM full 0.8072727\n7           GBM selected 0.7090909\n\n\nIn conclusion Logistic Regression Model seems to be performing the best."
  }
]